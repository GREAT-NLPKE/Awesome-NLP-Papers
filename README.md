# Awesome-NLP-Papers

- [Awesome-NLP-Papers](#awesome-nlp-papers)
  - [For Beginners](#for-beginners)
  - [Backbone](#backbone)
  - [Large Model](#large-model)
  - [Training Method](#training-method)
  - [Downstream Tasks](#downstream-tasks)
    - [Named Entity Recognition](#named-entity-recognition)
    - [Relation Extraction](#relation-extraction)
    - [Open Information Extraction](#open-information-extraction)
  - [Report](#report)


## For Beginners
- **Transformer开山之作**：[*Attention Is All You Need*](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- **NLU预训练编码器开山之作**：[*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/pdf/1810.04805.pdf)
- **生成式预训练模型GPT**：[*Improving Language Understanding by Generative Pre-Training*](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- 结合encoder和decoder的通用语言模型：[*GLM: General Language Model Pretraining with Autoregressive Blank Infilling*](https://arxiv.org/pdf/2103.10360.pdf)



## Backbone
## Large Model
1. GLM: General Language Model Pretraining with Autoregressive Blank Infilling
- ACL 2022
- Tsinghua University, BAAI, MIT CSAIL, Shanghai Qi Zhi Institute
- [Arxiv](https://arxiv.org/pdf/2103.10360.pdf), Github: [v1](https://github.com/THUDM/GLM), [v2](https://github.com/THUDM/ChatGLM2-6B), [v3](https://github.com/THUDM/ChatGLM3), [GLM-130B](https://github.com/THUDM/GLM-130B)
## Training Method
1. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
- ACL 2022
- Tsinghua University, BAAI, Shanghai Qi Zhi Institute
- [Arxiv](https://arxiv.org/pdf/2110.07602.pdf), [Github](https://github.com/THUDM/P-tuning-v2)
2. LoRA: Low-Rank Adaptation of Large Language Models
- ICLR 2022
- Microsoft
- [Arxiv](https://arxiv.org/pdf/2106.09685.pdf), [Github](https://github.com/microsoft/LoRA)

## Downstream Tasks
### Named Entity Recognition
1. Parallel Instance Query Network for Named Entity Recognition
- ACL 2022
- Zhejiang University, DAMO
- [Arxiv](https://arxiv.org/pdf/2203.10545v1.pdf), [Github](https://github.com/tricktreat/piqn)

### Relation Extraction
1. DREEAM: Guiding Attention with Evidence for Improving Document-Level Relation Extraction
- ACL 2023
- Tokyo Institute of Technology
- [Arxiv](https://arxiv.org/pdf/2302.08675v1.pdf), [Github](https://github.com/YoumiMa/dreeam)

### Open Information Extraction
1. Unified Structure Generation for Universal Information Extraction
- ACL 2022
- Chinese Information Processing Laboratory, Chinese Academy of Sciences, Baidu Inc., University of Chinese Academy of Sciences, Beijing Academy of Artificial Intelligence
- [Arxiv](https://arxiv.org/pdf/2203.12277.pdf),[Github](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/uie)
- 
## Report
1. AcademicGPT: Empowering Academic Research
- IDEA ReadPaper Team
- [Arxiv](https://arxiv.org/pdf/2311.12315.pdf)