# Awesome-NLP-Papers
## For Beginners
- **Transformer开山之作**：[*Attention Is All You Need*](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- **NLU预训练编码器开山之作**：[*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/pdf/1810.04805.pdf)
- **生成式预训练模型GPT**：[*Improving Language Understanding by Generative Pre-Training*](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- 结合encoder和decoder的通用语言模型：[*GLM: General Language Model Pretraining with Autoregressive Blank Infilling*](https://arxiv.org/pdf/2103.10360.pdf)

## Backbone
## Large Model
1. GLM: General Language Model Pretraining with Autoregressive Blank Infilling
- ACL 2022
- Tsinghua University, BAAI, MIT CSAIL, Shanghai Qi Zhi Institute
- [Arxiv](https://arxiv.org/pdf/2103.10360.pdf), Github: [v1](https://github.com/THUDM/GLM), [v2](https://github.com/THUDM/ChatGLM2-6B), [v3](https://github.com/THUDM/ChatGLM3), [GLM-130B](https://github.com/THUDM/GLM-130B)
## Training Method
1. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
- ACL 2022
- Tsinghua University, BAAI, Shanghai Qi Zhi Institute
- [Arxiv](https://arxiv.org/pdf/2110.07602.pdf), [Github](https://github.com/THUDM/P-tuning-v2)
2. LoRA: Low-Rank Adaptation of Large Language Models
- ICLR 2022
- Microsoft
- [Arxiv](https://arxiv.org/pdf/2106.09685.pdf), [Github](https://github.com/microsoft/LoRA)

## Downstream Tasks
### Named Entity Recognition
### Relation Extraction
## Others